import streamlit as st
import plotly.graph_objects as go

from config import CONFIG
from core import (
    SpacyTextProcessor,
    VocabularyProfile,
    BinarySearchVocabularyTester,
    build_difficulty_curve,
    comfort_level_from_unknown_share,
    TextProcessingError,
)


st.set_page_config(
    page_title="–°–ª–æ–∂–Ω–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è –∫–Ω–∏–≥–∏",
    layout="wide",
)


# ---------- —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------

def init_session_state():
    if "processor" not in st.session_state:
        st.session_state["processor"] = SpacyTextProcessor()
    if "processed_text" not in st.session_state:
        st.session_state["processed_text"] = None
    if "vocab" not in st.session_state:
        st.session_state["vocab"] = None
    if "tester" not in st.session_state:
        st.session_state["tester"] = None
    if "current_question" not in st.session_state:
        st.session_state["current_question"] = None


def make_curve_figure(df):
    """–°—Ç—Ä–æ–∏–º plotly‚Äë–≥—Ä–∞—Ñ–∏–∫ —Å —Ü–≤–µ—Ç–Ω—ã–º–∏ –∑–æ–Ω–∞–º–∏ –∫–æ–º—Ñ–æ—Ä—Ç–∞."""
    if df.empty:
        return go.Figure()

    x = df["position_frac"] * 100
    y = df["unknown_ratio_smooth"] * 100

    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=x,
            y=y,
            mode="lines",
            name="–ù–µ–∑–Ω–∞–∫–æ–º—ã–µ —Å–ª–æ–≤–∞ (—Å–≥–ª–∞–∂–µ–Ω–æ)",
        )
    )

    green_max = CONFIG.comfort_green_max_unknown * 100
    yellow_max = CONFIG.comfort_yellow_max_unknown * 100
    y_max = max(y.max() if len(y) else 0, yellow_max * 1.5)

    # —Ü–≤–µ—Ç–Ω—ã–µ –∑–æ–Ω—ã
    fig.add_shape(
        type="rect",
        x0=0,
        x1=100,
        y0=0,
        y1=green_max,
        fillcolor="green",
        opacity=0.15,
        line_width=0,
        layer="below",
    )
    fig.add_shape(
        type="rect",
        x0=0,
        x1=100,
        y0=green_max,
        y1=yellow_max,
        fillcolor="yellow",
        opacity=0.15,
        line_width=0,
        layer="below",
    )
    fig.add_shape(
        type="rect",
        x0=0,
        x1=100,
        y0=yellow_max,
        y1=y_max,
        fillcolor="red",
        opacity=0.15,
        line_width=0,
        layer="below",
    )

    fig.update_layout(
        xaxis_title="–ü–æ–∑–∏—Ü–∏—è –≤ –∫–Ω–∏–≥–µ, %",
        yaxis_title="–î–æ–ª—è –Ω–µ–∑–Ω–∞–∫–æ–º—ã—Ö —Å–ª–æ–≤, %",
        yaxis_range=[0, y_max],
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=40, r=20, t=40, b=40),
    )
    return fig


# ---------- UI ----------

init_session_state()

st.title("–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —á—Ç–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –∫–Ω–∏–≥–∏")
st.markdown(
    """
–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, **–∫–∞–∫—É—é –¥–æ–ª—é —Å–ª–æ–≤** –∫–Ω–∏–≥–∏ –≤—ã, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ,
–Ω–µ –∑–Ω–∞–µ—Ç–µ, –∏ —Å—Ç—Ä–æ–∏—Ç –∫—Ä–∏–≤—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ —Ö–æ–¥—É —Ç–µ–∫—Å—Ç–∞.

1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ `.txt` —Ñ–∞–π–ª –∫–Ω–∏–≥–∏.
2. –ü—Ä–æ–π–¥–∏—Ç–µ –∫–æ—Ä–æ—Ç–∫–∏–π **yes/no** —Ç–µ—Å—Ç –Ω–∞ –∑–Ω–∞–Ω–∏–µ —Å–ª–æ–≤.
3. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –≥—Ä–∞—Ñ–∏–∫.
"""
)

uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—Å—Ç –∫–Ω–∏–≥–∏ (.txt)", type=["txt"])

if uploaded is not None and st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç"):
    raw_bytes = uploaded.getvalue()
    text = raw_bytes.decode("utf-8", errors="ignore")
    processor = st.session_state["processor"]

    try:
        with st.spinner("–†–∞–∑–±–∏—Ä–∞—é —Ç–µ–∫—Å—Ç –∏ —Å—Ç—Ä–æ—é —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å..."):
            processed = processor.process(text)
            vocab = VocabularyProfile.from_processed_text(processed)
    except TextProcessingError as exc:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {exc}")
    else:
        st.session_state["processed_text"] = processed
        st.session_state["vocab"] = vocab
        st.session_state["tester"] = None
        st.session_state["current_question"] = None

        st.success(
            f"–ì–æ—Ç–æ–≤–æ! –í —Ç–µ–∫—Å—Ç–µ {len(processed.tokens)} —Ç–æ–∫–µ–Ω–æ–≤, "
            f"{vocab.total_tokens} —Ç–æ–∫–µ–Ω–æ–≤ —É—á—Ç–µ–Ω–æ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ, "
            f"{len(vocab)} —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ª–µ–º–º (–±–µ–∑ –∏–º—ë–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö)."
        )

processed = st.session_state["processed_text"]
vocab = st.session_state["vocab"]

if processed and vocab:
    st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("–¢–æ–∫–µ–Ω–æ–≤ (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏)", len(processed.tokens))
    with col2:
        st.metric("–õ–µ–º–º –≤ —á–∞—Å—Ç–æ—Ç–Ω–∏–∫–µ", len(vocab))
    with col3:
        st.metric("–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏", vocab.total_tokens)

    st.markdown("---")
    st.subheader("–¢–µ—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å")

    # –∑–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞
    if st.session_state["tester"] is None:
        if st.button("–ù–∞—á–∞—Ç—å —Ç–µ—Å—Ç"):
            st.session_state["tester"] = BinarySearchVocabularyTester(
                vocab=vocab,
                batch_size=CONFIG.bs_batch_size,
                max_questions=CONFIG.max_questions,
            )
            st.session_state["current_question"] = None
            st.rerun()

tester: BinarySearchVocabularyTester | None = st.session_state.get("tester")

if tester and processed and vocab:
    if not tester.is_finished:
        st.info(
            "–û—Ç–º–µ—á–∞–π—Ç–µ **–¥–∞/–Ω–µ—Ç**, –∏—Å—Ö–æ–¥—è –∏–∑ —Ç–æ–≥–æ, –∑–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã —Å–ª–æ–≤–æ "
            "–≤ —Ç–∏–ø–∏—á–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —á—Ç–µ–Ω–∏—è."
        )

        progress = tester.question_count / CONFIG.max_questions
        st.progress(
            progress,
            text=f"–í–æ–ø—Ä–æ—Å–æ–≤ –∑–∞–¥–∞–Ω–æ: {tester.question_count} / {CONFIG.max_questions}",
        )

        if st.session_state["current_question"] is None:
            # –ó–¥–µ—Å—å next_question –º–æ–∂–µ—Ç —Å—Ä–∞–∑—É –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ç–µ—Å—Ç –∏ –≤–µ—Ä–Ω—É—Ç—å None
            q = tester.next_question()
            st.session_state["current_question"] = q

            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –≤—ã–∑–æ–≤–∞ next_question —Ç–µ—Å—Ç —É–∂–µ –∑–∞–≤–µ—Ä—à—ë–Ω,
            # —Å—Ä–∞–∑—É –ø–µ—Ä–µ—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏ —É–π–¥—ë–º –≤ –≤–µ—Ç–∫—É "tester.is_finished"
            if q is None and tester.is_finished:
                st.rerun()
        else:
            q = st.session_state["current_question"]

        if q is None:
            st.info("–ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–ª—É—á–∏–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ—Å—Ç.")
        else:
            st.markdown(
                f"### –ó–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã —ç—Ç–æ —Å–ª–æ–≤–æ?\n\n"
                f"**`{q.lemma}`**"
            )
            c1, c2 = st.columns(2)
            if c1.button("–î–∞, –∑–Ω–∞—é", key=f"yes_{q.vocab_index}_{tester.question_count}"):
                tester.record_answer(q.vocab_index, True)
                st.session_state["current_question"] = None
                st.rerun()
            if c2.button("–ù–µ—Ç, –Ω–µ –∑–Ω–∞—é", key=f"no_{q.vocab_index}_{tester.question_count}"):
                tester.record_answer(q.vocab_index, False)
                st.session_state["current_question"] = None
                st.rerun()
    else:
        # <-- —ç—Ç–∞ —á–∞—Å—Ç—å —É–∂–µ –µ—Å—Ç—å: –∑–¥–µ—Å—å —Å—á–∏—Ç–∞—é—Ç—Å—è known/unknown –∏ —Ä–∏—Å—É–µ—Ç—Å—è –≥—Ä–∞—Ñ–∏–∫
        st.success("–¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω üéâ")

        threshold_index = tester.estimated_threshold_index
        if threshold_index is None:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –ø–æ—Ä–æ–≥ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞.")
        else:
            known_share = vocab.known_token_share(threshold_index)
            unknown_share = vocab.unknown_token_share(threshold_index)
            level = comfort_level_from_unknown_share(
                unknown_share,
                CONFIG.comfort_green_max_unknown,
                CONFIG.comfort_yellow_max_unknown,
            )

            st.markdown(
                f"""
**–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è —ç—Ç–æ–π –∫–Ω–∏–≥–∏**

* –ó–Ω–∞–µ—Ç–µ –ø—Ä–∏–º–µ—Ä–Ω–æ **{known_share * 100:.1f}%** —Å–ª–æ–≤–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π.
* –ù–µ–∑–Ω–∞–∫–æ–º—ã—Ö ‚Äî **{unknown_share * 100:.1f}%** (–∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ —Å—á–∏—Ç–∞—é—Ç—Å—è).
"""
            )

            if level == "green":
                st.success("–ó–µ–ª—ë–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: –∫–Ω–∏–≥—É –±—É–¥–µ—Ç –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ —á–∏—Ç–∞—Ç—å.")
            elif level == "yellow":
                st.warning(
                    "–ñ—ë–ª—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å: —á—Ç–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–æ –±—É–¥–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å —É—Å–∏–ª–∏–π."
                )
            else:
                st.error(
                    "–ö—Ä–∞—Å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: –∫–Ω–∏–≥–∞ –±—É–¥–µ—Ç —Ç—è–∂—ë–ª–æ–π –¥–ª—è —á—Ç–µ–Ω–∏—è, "
                    "–º–Ω–æ–≥–æ –Ω–µ–∑–Ω–∞–∫–æ–º—ã—Ö —Å–ª–æ–≤."
                )

            st.subheader("–ö—Ä–∏–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ —Ö–æ–¥—É –∫–Ω–∏–≥–∏")
            df_curve = build_difficulty_curve(
                processed,
                vocab,
                threshold_index,
                CONFIG.segment_token_size,
                CONFIG.smoothing_window,
            )
            fig = make_curve_figure(df_curve)
            st.plotly_chart(fig, use_container_width=True)

            st.caption(
                "–¶–≤–µ—Ç–Ω—ã–µ –∑–æ–Ω—ã –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –ø–æ lexical coverage: "
                "–∑–µ–ª—ë–Ω–∞—è –∑–æ–Ω–∞ ‚âà 98‚Äì99% –∑–Ω–∞–∫–æ–º—ã—Ö —Å–ª–æ–≤, –∂—ë–ª—Ç–∞—è ‚Äî 95‚Äì98%, "
                "–∫—Ä–∞—Å–Ω–∞—è ‚Äî –Ω–∏–∂–µ 95% –∑–Ω–∞–∫–æ–º—ã—Ö —Å–ª–æ–≤."
            )
